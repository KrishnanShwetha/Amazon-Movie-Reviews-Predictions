{"cells":[{"cell_type":"markdown","metadata":{"id":"QdfCfPvPdPW6"},"source":["# Testing Models"]},{"cell_type":"markdown","metadata":{"id":"TkhNjo_cdPW9"},"source":["## (2) KNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CA4kWthOdPW9","outputId":"ef83198b-1e90-4492-8059-f2c3265597ea"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>HelpfulnessNumerator</th>\n","      <th>HelpfulnessDenominator</th>\n","      <th>Score</th>\n","      <th>Time</th>\n","      <th>neg</th>\n","      <th>neu</th>\n","      <th>pos</th>\n","      <th>compound</th>\n","      <th>Helpfulness</th>\n","      <th>UnHelpfulness</th>\n","      <th>UserAvgScore</th>\n","      <th>NumUppercase_T</th>\n","      <th>NumUppercase_S</th>\n","      <th>ReviewLength</th>\n","      <th>SummaryLength</th>\n","      <th>Month</th>\n","      <th>Year</th>\n","      <th>numExclamation_S</th>\n","      <th>numExclamation_T</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1561604</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>2.0</td>\n","      <td>1388793600</td>\n","      <td>0.153</td>\n","      <td>0.692</td>\n","      <td>0.155</td>\n","      <td>-0.4040</td>\n","      <td>0.250000</td>\n","      <td>0.250000</td>\n","      <td>2.000000</td>\n","      <td>9</td>\n","      <td>2</td>\n","      <td>97</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>2014</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>516128</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>5.0</td>\n","      <td>1033171200</td>\n","      <td>0.000</td>\n","      <td>0.727</td>\n","      <td>0.273</td>\n","      <td>0.9231</td>\n","      <td>0.714286</td>\n","      <td>0.714286</td>\n","      <td>3.083333</td>\n","      <td>6</td>\n","      <td>5</td>\n","      <td>43</td>\n","      <td>5</td>\n","      <td>9</td>\n","      <td>2002</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1265775</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4.0</td>\n","      <td>1390521600</td>\n","      <td>0.000</td>\n","      <td>0.767</td>\n","      <td>0.233</td>\n","      <td>0.7901</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>4.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>25</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2014</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>484323</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>4.0</td>\n","      <td>1136332800</td>\n","      <td>0.030</td>\n","      <td>0.771</td>\n","      <td>0.199</td>\n","      <td>0.9829</td>\n","      <td>0.750000</td>\n","      <td>0.750000</td>\n","      <td>4.500000</td>\n","      <td>14</td>\n","      <td>1</td>\n","      <td>126</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>2006</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>158350</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>5.0</td>\n","      <td>1372204800</td>\n","      <td>0.000</td>\n","      <td>0.880</td>\n","      <td>0.120</td>\n","      <td>0.4588</td>\n","      <td>0.500000</td>\n","      <td>0.500000</td>\n","      <td>4.600000</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>23</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>2013</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>192763</td>\n","      <td>5</td>\n","      <td>8</td>\n","      <td>5.0</td>\n","      <td>1138579200</td>\n","      <td>0.035</td>\n","      <td>0.833</td>\n","      <td>0.132</td>\n","      <td>0.9938</td>\n","      <td>0.625000</td>\n","      <td>0.625000</td>\n","      <td>4.500000</td>\n","      <td>188</td>\n","      <td>27</td>\n","      <td>440</td>\n","      <td>17</td>\n","      <td>1</td>\n","      <td>2006</td>\n","      <td>0</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1060454</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4.0</td>\n","      <td>1403654400</td>\n","      <td>0.111</td>\n","      <td>0.737</td>\n","      <td>0.152</td>\n","      <td>0.4118</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>3.500000</td>\n","      <td>9</td>\n","      <td>2</td>\n","      <td>54</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>2014</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>860129</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>3.0</td>\n","      <td>1162080000</td>\n","      <td>0.151</td>\n","      <td>0.754</td>\n","      <td>0.095</td>\n","      <td>-0.6306</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>4.000000</td>\n","      <td>13</td>\n","      <td>3</td>\n","      <td>46</td>\n","      <td>3</td>\n","      <td>10</td>\n","      <td>2006</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1470316</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>1.0</td>\n","      <td>1357776000</td>\n","      <td>0.030</td>\n","      <td>0.813</td>\n","      <td>0.157</td>\n","      <td>0.7884</td>\n","      <td>0.400000</td>\n","      <td>0.400000</td>\n","      <td>1.000000</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>41</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2013</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>725990</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>5.0</td>\n","      <td>1396828800</td>\n","      <td>0.022</td>\n","      <td>0.800</td>\n","      <td>0.178</td>\n","      <td>0.9504</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>5.000000</td>\n","      <td>22</td>\n","      <td>1</td>\n","      <td>87</td>\n","      <td>5</td>\n","      <td>4</td>\n","      <td>2014</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        Id  HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n","0  1561604                     1                       4    2.0  1388793600   \n","1   516128                     5                       7    5.0  1033171200   \n","2  1265775                     0                       0    4.0  1390521600   \n","3   484323                     3                       4    4.0  1136332800   \n","4   158350                     1                       2    5.0  1372204800   \n","5   192763                     5                       8    5.0  1138579200   \n","6  1060454                     0                       0    4.0  1403654400   \n","7   860129                     0                       6    3.0  1162080000   \n","8  1470316                     2                       5    1.0  1357776000   \n","9   725990                     0                       0    5.0  1396828800   \n","\n","     neg    neu    pos  compound  Helpfulness  UnHelpfulness  UserAvgScore  \\\n","0  0.153  0.692  0.155   -0.4040     0.250000       0.250000      2.000000   \n","1  0.000  0.727  0.273    0.9231     0.714286       0.714286      3.083333   \n","2  0.000  0.767  0.233    0.7901     0.000000       0.000000      4.000000   \n","3  0.030  0.771  0.199    0.9829     0.750000       0.750000      4.500000   \n","4  0.000  0.880  0.120    0.4588     0.500000       0.500000      4.600000   \n","5  0.035  0.833  0.132    0.9938     0.625000       0.625000      4.500000   \n","6  0.111  0.737  0.152    0.4118     0.000000       0.000000      3.500000   \n","7  0.151  0.754  0.095   -0.6306     0.000000       0.000000      4.000000   \n","8  0.030  0.813  0.157    0.7884     0.400000       0.400000      1.000000   \n","9  0.022  0.800  0.178    0.9504     0.000000       0.000000      5.000000   \n","\n","   NumUppercase_T  NumUppercase_S  ReviewLength  SummaryLength  Month  Year  \\\n","0               9               2            97              3      1  2014   \n","1               6               5            43              5      9  2002   \n","2               0               0            25              1      1  2014   \n","3              14               1           126              3      1  2006   \n","4               0               1            23              2      6  2013   \n","5             188              27           440             17      1  2006   \n","6               9               2            54              4      6  2014   \n","7              13               3            46              3     10  2006   \n","8               3               1            41              1      1  2013   \n","9              22               1            87              5      4  2014   \n","\n","   numExclamation_S  numExclamation_T  \n","0                 1                 0  \n","1                 1                 0  \n","2                 0                 1  \n","3                 1                 3  \n","4                 0                 0  \n","5                 0                 4  \n","6                 0                 0  \n","7                 0                 0  \n","8                 0                 0  \n","9                 0                 0  "]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# import\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Load the dataset - new data set after feature extraction\n","train = pd.read_csv(\"new_feauturesExtracted_dataset.csv\")\n","\n","train.head(10)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PmrAtTRAdPW_","outputId":"7f44eec5-027c-4539-f420-2bc50fac994a"},"outputs":[{"name":"stdout","output_type":"stream","text":["(111748, 21) (27936, 21)\n"]}],"source":["# we have to split this data into train and test\n","# We create a new column called `train` which is `True` if the instance should be included in the training by using the numpy random number generator.\n","\n","train['train'] = np.random.rand(len(train)) < 0.8\n","\n","df_train = train[train.train == True]\n","df_test = train[train.train == False]\n","\n","print(df_train.shape, df_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vqXl4n0-dPXA","outputId":"cb048b2b-1cd7-40da-d876-794ec19b4f69"},"outputs":[{"data":{"text/plain":["Index(['Id', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Time',\n","       'neg', 'neu', 'pos', 'compound', 'Helpfulness', 'UnHelpfulness',\n","       'UserAvgScore', 'NumUppercase_T', 'NumUppercase_S', 'ReviewLength',\n","       'SummaryLength', 'Month', 'Year', 'numExclamation_S',\n","       'numExclamation_T', 'train'],\n","      dtype='object')"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["train.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4cCWGwaFdPXA"},"outputs":[],"source":["predictors = [ 'neg', 'pos', 'compound', 'Helpfulness', 'UnHelpfulness',\n","       'UserAvgScore', 'NumUppercase_T', 'NumUppercase_S', 'ReviewLength',\n","       'SummaryLength', 'Month', 'numExclamation_S',\n","       'numExclamation_T']\n","X1_train = df_train[predictors]\n","X1_test = df_test[predictors]\n","y_train = df_train['Score']\n","y_test = df_test['Score']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y2bylgXsdPXA","outputId":"2030f1a2-fa5c-469d-f566-7358c8d3a688"},"outputs":[{"data":{"text/html":["<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsRegressor</label><div class=\"sk-toggleable__content\"><pre>KNeighborsRegressor()</pre></div></div></div></div></div>"],"text/plain":["KNeighborsRegressor()"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn import neighbors     \n","\n","# choose model class\n","n_neighbors = 5\n","# instantiate model\n","model = neighbors.KNeighborsRegressor(n_neighbors) \n","# fit model to data    \n","model.fit(X1_train, y_train)                            "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xUaqNLBudPXB","outputId":"c1d69802-44d0-4531-aa7d-14a43aa036a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["predictions on training set:\n","0.8285365278829268 0.6864727780362959\n"]}],"source":["# predict on training data\n","print(\"predictions on training set:\")\n","y_train_fit = model.predict(X1_train)        \n","mse_train = np.mean( (y_train - y_train_fit)**2 )\n","print(np.sqrt(mse_train), mse_train)    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fxQR4Wj2dPXB","outputId":"a6b5a28c-0d78-4e25-9f37-e82cd6a74f20"},"outputs":[{"name":"stdout","output_type":"stream","text":["predictions on test set:\n","1.0312208203376343 1.0634163802978236\n"]}],"source":["# predict on test data\n","print(\"predictions on test set:\")\n","y_test_fit = model.predict(X1_test)                     \n","mse_test = np.mean( (y_test - y_test_fit)**2 )\n","print(np.sqrt(mse_test), mse_test)"]},{"cell_type":"markdown","metadata":{"id":"zMhalZxVdPXC"},"source":["This means that, on average, the model's predictions are off by 0.6864727780362959 for the training set and 1.0634163802978236 for the testing set. This indicates that the model performs better on the training set than on the testing set. But, since the difference between the RMSEs is not very large, which suggests that the model is not overfitting the training data."]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}