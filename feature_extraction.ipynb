{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ShwethaKrishnan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/ShwethaKrishnan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import random\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "string.punctuation\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def process(df):\n",
    "    # This is where you can do all your processing\n",
    "\n",
    "    # (0) Drop rows where helpfullness numerator is more than denominator\n",
    "    df = df[df.HelpfulnessNumerator <= df.HelpfulnessDenominator ]\n",
    "\n",
    "    # (1) Helpfulness - feature extraction\n",
    "    df['Helpfulness'] = df['HelpfulnessNumerator'] / df['HelpfulnessDenominator']\n",
    "    df['Helpfulness'] = df['Helpfulness'].fillna(0)\n",
    "\n",
    "    # (2) Unhelpfulness - feature extraction\n",
    "    df['UnHelpfulness'] = (df['HelpfulnessDenominator'] - df['HelpfulnessNumerator'])/ df['HelpfulnessDenominator']\n",
    "    df['UnHelpfulness'] = df['Helpfulness'].fillna(0)\n",
    "\n",
    "    # (3) Average Score written by each UserId\n",
    "    df['UserAvgScore'] = df.groupby('UserId')['Score'].transform('mean')\n",
    "    df['UserAvgScore'] = df['UserAvgScore'].fillna(0)\n",
    "\n",
    "    \"\"\"\n",
    "    # (3) Average Score for each product\n",
    "    df['ProductAvgScore'] = df.groupby('ProductId')['Score'].transform('mean')\n",
    "    \"\"\"\n",
    "\n",
    "    # (4) counting the number of uppercase characters in reviews\n",
    "    df['NumUppercase_T'] = df['Text'].str.findall(r'[A-Z]').str.len()\n",
    "    df['NumUppercase_S'] = df['Summary'].str.findall(r'[A-Z]').str.len()\n",
    "\n",
    "    # (6) ReviewLength - feature extraction\n",
    "    df['ReviewLength'] = df.Text.str.split().str.len()\n",
    "    df['ReviewLength'] = df['ReviewLength'].fillna(0)\n",
    "\n",
    "    # (7) SummaryLength - feature extraction\n",
    "    df['SummaryLength'] = df.Summary.str.split().str.len()\n",
    "    df['SummaryLength'] = df['SummaryLength'].fillna(0)\n",
    "\n",
    "    # (8) Time Stamps - feature extraction\n",
    "    df['Date'] = pd.to_datetime(trainingSet['Time'], unit='s')\n",
    "    #df['Hour'] = df['Date'].dt.hour\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "\n",
    "    # (9) Counting the number of times an exclamation occurs in a review - feature extraction\n",
    "    sub = \"!\"\n",
    "    df['numExclamation_S'] = df['Summary'].str.count(sub)\n",
    "    df['numExclamation_T'] = df['Text'].str.count(sub)\n",
    "\n",
    "    # tf-idf vectorization of string \n",
    "    tfidf = TfidfVectorizer(ngram_range = (1,3), input='content', analyzer='word', stop_words='english', min_df=0.00001, max_df=0.5, max_features=150000)\n",
    "    tfidf_encodings = tfidf.fit_transform(df['Text'])\n",
    "    df['TFIDFText'] = list(tfidf_encodings.toarray())\n",
    "\n",
    "    \"\"\"\n",
    "    # (10) Counting the number of times a question mark occurs in a review - feature extraction\n",
    "    df['numQuestions_S'] = df['Summary'].apply(lambda x: len(re.findall(\"?\", x)))\n",
    "    df['numQuestion_T'] = df['Text'].apply(lambda x: len(re.findall(\"?\", x)))\n",
    "    \n",
    "\n",
    "    # (11) removing punctuations (pre-processing step)\n",
    "    df['Summary']= df['Summary'].apply(lambda x:remove_punctuation(x))\n",
    "    df['Text']= df['Text'].apply(lambda x:remove_punctuation(x))\n",
    "\n",
    "    # (12) converting everything to lower case (pre-processing step)\n",
    "    df['Summary']= df['Summary'].apply(lambda x: x.lower())\n",
    "    df['Text']= df['Text'].apply(lambda x: x.lower())\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # (13) Tokenization (pre-processing step)\n",
    "    df['Summary']= df['Summary'].apply(lambda x: word_tokenize(x))\n",
    "    df['Text']= df['Text'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "    # (14) removing stop words (pre-processing step)\n",
    "    df['Summary']= df['Summary'].apply(lambda x: remove_stopwords(x))\n",
    "    df['Text']= df['Text'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "    # (15) Stemming (pre-processing step)\n",
    "    df['Summary']= df['Summary'].apply(lambda x: stemming_text(x))\n",
    "    df['Text']= df['Text'].apply(lambda x: stemming_text(x))\n",
    "\n",
    "    # (16) cleaned text length - summary and text length after removing stop words\n",
    "    df['CleanSummaryLength'] = df['Summary'].apply(lambda x: len(x))\n",
    "    df['CleanSummaryLength'] = df['CleanSummaryLength'].fillna(0)\n",
    "\n",
    "    df['CleanReviewLength'] = df['Text'].apply(lambda x: len(x))\n",
    "    df['CleanReviewLength'] = df['CleanReviewLength'].fillna(0)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # (17) Lemmatization of text (pre-processing step)\n",
    "    df['Summary']= df['Summary'].apply(lambda x: lemmatizing_text(x))\n",
    "    df['Text']= df['Text'].apply(lambda x: lemmatizing_text(x))\n",
    "    \"\"\"\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "trainingSet = pd.read_csv(\"./data/train.csv\")\n",
    "\n",
    "# Reducing sample size to 10% - random sampling \n",
    "trainingSet = trainingSet.sample(frac=0.1)\n",
    "#trainingSet = trainingSet.head(50)\n",
    "\n",
    "# Handeling missing values \n",
    "# (1) Remove rows with null score values \n",
    "trainingSet = trainingSet[trainingSet['Score'].notnull()]\n",
    "\n",
    "\n",
    "# text pre-processing :\n",
    "\n",
    "# (1) converting all float values to strings in summary and text column\n",
    "trainingSet[\"Summary\"] = trainingSet[\"Summary\"].values.astype('str')\n",
    "trainingSet[\"Text\"] = trainingSet[\"Text\"].values.astype('str')\n",
    "\n",
    "# (2) removing punctuations \n",
    "def remove_punctuation(text):\n",
    "    punctuation_removed = \"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuation_removed\n",
    "\n",
    "# (4) removing stop words\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    stopwords_removed= [i for i in text if i not in stopwords]\n",
    "    return stopwords_removed\n",
    "\n",
    "# (5) stemming of text\n",
    "snow = SnowballStemmer(language='english')\n",
    "def stemming_text(text):\n",
    "    stemmed_text = [snow.stem(word) for word in text]\n",
    "    return stemmed_text\n",
    "\"\"\"\n",
    "\n",
    "# (6) Lemmantization of text\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatizing_text(text):\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return lemmatized_text\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# further feature extraction - sentiment analysis and tf-idf vectorization\n",
    "\n",
    "# sentiment analysis on Review Text\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "res = {}\n",
    "for i, row in trainingSet.iterrows():\n",
    "    text = row['Text']\n",
    "    myid = row['Id']\n",
    "    res[myid] = sia.polarity_scores(text)\n",
    "\n",
    "vaders = pd.DataFrame(res).T\n",
    "vaders = vaders.reset_index().rename(columns = {'index':\"Id\"})\n",
    "trainingSet = pd.merge(trainingSet, vaders, left_on='Id', right_on='Id')\n",
    "\n",
    "# tf-idf vectorization\n",
    "tfidf = TfidfVectorizer(ngram_range = (1,3), input='content', analyzer='word', stop_words='english', min_df=0.00001, max_df=0.5, max_features=150000)\n",
    "tfidf_encodings = tfidf.fit_transform(trainingSet['Text'])\n",
    "trainingSet['TFIDFText'] = list(tfidf_encodings.toarray())\n",
    "\n",
    "\n",
    "\n",
    "# Standardize the numerical features\n",
    "\n",
    "# using standard scalar - reduces the influence of outliers and helps converge faster\n",
    "\"\"\"\n",
    "scaler = StandardScaler()\n",
    "trainingSet[['Helpfulness', 'UnHelpfulness', 'ReviewLength', 'SummaryLength']] = scaler.fit_transform(trainingSet[['Helpfulness', 'UnHelpfulness', 'ReviewLength', 'SummaryLength']])\n",
    "trainingSet[['Helpfulness', 'UnHelpfulness', 'ReviewLength', 'SummaryLength']] = scaler.fit_transform(trainingSet[['Helpfulness', 'UnHelpfulness', 'ReviewLength', 'SummaryLength']])\n",
    "\"\"\"\n",
    "\n",
    "# Process the DataFrame\n",
    "train_processed = process(trainingSet)\n",
    "\n",
    "\n",
    "# Load test set\n",
    "submissionSet = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "\n",
    "# Merge on Id so that the test set can have feature columns as well\n",
    "testX= pd.merge(train_processed, submissionSet, left_on='Id', right_on='Id')\n",
    "testX = testX.drop(columns=['Score_x'])\n",
    "testX = testX.rename(columns={'Score_y': 'Score'})\n",
    "\n",
    "# The training set is where the score is not null\n",
    "trainX =  train_processed[train_processed['Score'].notnull()]\n",
    "\n",
    "# X_test.csv is test.csv with features extracted from train.csv and other features added while generating features\n",
    "testX.to_csv(\"./data/X_test.csv\", index=False)\n",
    "trainX.to_csv(\"./data/X_train.csv\", index=False)\n",
    "\n",
    "# runtime(f 1-1) : 1m 25.9s\n",
    "# runtime(f 1-3) : 15m 37.4s\n",
    "# runtime(f 1-3) : 26m 7.5s\n",
    "# runtime with sentiment analysis : 24m 54.2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "5     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "6     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "7     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "8     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "9     [0.09490090756715851, 0.09490090756715851, 0.0...\n",
       "10    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "11    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "12    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "13    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "14    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "15    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "16    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "17    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "18    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "19    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "20    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "21    [0.0, 0.0, 0.0, 0.11532855186557939, 0.1153285...\n",
       "22    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "23    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "24    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "Name: TFIDFText, dtype: object"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see processed dataset\n",
    "#trainingSet.head(25)\n",
    "#train_processed.head(25)\n",
    "train_processed['TFIDFText'].head(25)\n",
    "#submissionSet.shape\n",
    "#test = pd.read_csv(\"./data/test.csv\")\n",
    "#test.shape\n",
    "#testX.shape\n",
    "#trainX.shape\n",
    "#ts.shape\n",
    "#train_processed.shape\n",
    "#submissionSet.shape\n",
    "# runtime 0.1s\n",
    "\n",
    "#train_processed['ProductAvgScore'].head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
